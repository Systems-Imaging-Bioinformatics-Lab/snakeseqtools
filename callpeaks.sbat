#!/bin/bash

#SBATCH --job-name=snakeseqtools	## Name of the job for the scheduler
#SBATCH --account=ukarvind99	 ## name of the resource account (who is paying for the compute time)
#SBATCH --partition=standard	## name of the queue to submit the job to. (Choose from: standard, debug, largemem, gpu   ---  accordingly)
#SBATCH --nodes=1                      ## number of nodes you are requesting
#SBATCH --ntasks=1                     ## how many tasks (resource spaces) do you want to reserve
#SBATCH --cpus-per-task=16              ## how many cores do you want for each task
#SBATCH --time=00:30:00                 ## Maximum length of time you are reserving the resources for (if job ends sooner, bill is based on time used)
#SBATCH --mem=40g                       ## Memory requested per core
#SBATCH --mail-user=anashank@umich.edu   ## send email notifications to umich email listed
#SBATCH --mail-type=END                ## when to send email (standard values are: NONE, BEGIN, END, FAIL, REQUEUE, ALL.  See documentation for others)
# I recommend using the following lines so that some output is put in your output file as an indicator your script is working

if [[ $SLURM_JOB_NODELIST ]] ; then
   echo "Running on"
   scontrol show hostnames $SLURM_JOB_NODELIST
fi


module load Bioinformatics
module load fastqc
module load trimgalore
module load bowtie2
module load hisat2
module load kallisto
module load picard-tools
module load bedtools2

snakemake --unlock --snakefile callpeaks.smk --configfile config/callpeak_config.yaml --cores 16
snakemake --snakefile callpeaks.smk --configfile config/callpeak_config.yaml --cores 16
